# **Agent SDKに関する調査**

    (2025.03.31 須子）

## **1\. Agent SDKとは？**

### **1.1 Agent SDKの定義と概要**

* **定義**: AIエージェント（自律的にタスクを実行するAIプログラム）を効率的に開発するためのソフトウェア開発キット
* AIエージェントは外部ツール/APIにアクセスし、複数ステップの処理を自律的に実行
* **特徴**: マルチターン対話管理、ツール呼び出し、タスクの自動実行を少ない労力で実現
* OpenAI、Microsoft、Googleなど主要企業が各社プラットフォーム向けに提供abc

  ### **1.2 Agent SDKの主な機能と用途**
* **主要機能**:

  * ツールの統合（ウェブ検索、DB問合せ、API呼出、コード実行）
  * マルチステップのタスク実行（エージェントループ）
  * 複数エージェントの協調（オーケストレーション、ハンドオフ）
  * ガードレール（安全・品質管理）
  * トレース（実行過程の記録・可視化）
* **主な用途**:

  * カスタマーサポートの自動化
  * 情報収集・調査業務
  * コンテンツ生成
  * コードアシスタント
  * 営業支援（セールスプロスペクティング）

  ### **1.3 主要企業のAgent SDK概要**
* **OpenAI**:

  * Pythonベース、最小限のプリミティブで強力な機能実現
  * 他社LLMモデルにも対応可能
  * 強み: 開発者フレンドリーな設計、オープンソースの拡張性
* **Microsoft**:

  * Microsoft 365 Agents SDKとして提供
  * 企業システム統合・マルチチャネル展開重視
  * 強み: Copilotプラットフォーム連携、エンタープライズ向け
* **Google**:

  * Vertex AI Agent Builder（ノーコード型開発ツール）
  * 強み: 事前構築された拡張機能・データ接続の豊富さ
* **その他**:

  * AWS: Amazon Bedrock Agents
  * オープンソース: LangChain、Hugging Face Transformers Agents

  ## **2\. OpenAI Agent SDKについて**

  ### **2.1 OpenAI Agent SDKの基本概念と4つの主要コンポーネント**
* **エージェント (Agents)**:

  * LLMに役割指示・使用可能ツール・設定を与えたもの
  * `Agent`クラスで表現（name、instructions、toolsを指定）
* **ツール（Tools):**

  * エージェントが利用する拡張機能や外部API
  * エージェントは適切なタイミングで定義されたツールを呼び出し結果を自身の応答に反映
* **ハンドオフ (Handoffs)**:

  * エージェント間のタスク引継ぎ機構
  * 典型例: トリアージエージェントが他の専門エージェントに振り分け
* **ガードレール (Guardrails)**:

  * 入出力の検証・フィルタリング機構
  * 入力/出力の2種類のガードレールを設定可能
  * 条件に該当時は例外（トリップワイヤー）発火でループ制御

  ### **2.2 ツールとして使える拡張機能の具体的例**
* **ツールの種類**:

  * ホステッドツール: OpenAI提供の組み込みツール
  * 関数ツール: 開発者定義のPython関数をツール化
  * エージェントをツールとして使用: 他エージェントを関数的に呼出
* **OpenAI提供の組み込みツール**:

  * WebSearchTool: インターネット検索
  * FileSearchTool: ベクトルストアによる情報検索
  * ComputerTool: ユーザーPC操作の自動化
* **カスタム関数ツール**:

  * `@function_tool`デコレータでPython関数をツール化
  * 関数名→ツール名、ドックストリング→説明文、型ヒント→スキーマを自動推論
  * 社内システム・APIを直接エージェントに統合可能

  ## **3\. リアルタイムWeb検索を使ったリサーチ・レポート作成エージェントの作成例**

  ### **3.1 開発対象**
* エージェントがユーザのリサーチテーマに合わせてインターネット検索を行い、最新のニュース記事・論文・公的統計データなどを収集。情報を要約してレポートを作成し、引用先や関連リンクまで提示する。

  ### **3.2  Agent SDKを用いない場合**
* **環境準備**: Google Colabで必要なライブラリをインストールし、OpenAI APIキーを設定。
* **検索ツールの実装**: 一般的なWeb検索を行い、関連情報を取得（Google Custom Search APIまたはBing Search APIを活用）。
* **データ収集と要約**: Webページのテキストを抽出し、GPT-4を使って要約。
* **レポート生成**: 章構成のある詳細なレポートを作成し、PDFとして整形。
* **レポート出力**: PDFをダウンロード可能にし、必要ならGoogle Driveに保存。

**ライブラリーのインストール**

| \# ライブラリのインストール（Colabマジックコマンドを使用）\!pip install openai requests beautifulsoup4 pdfkit  \!apt-get update \-y  \!apt-get install \-y wkhtmltopdf  \# インストール後のインポートimport openaiimport requestsfrom bs4 import BeautifulSoupimport pdfkitimport os |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**OpenAI APIキーの設定**

| \# OpenAI APIキーの設定（環境変数経由が望ましい）os.environ\["OPENAI\_API\_KEY"\] \= "sk-XXXXXXXXXXXXXXXXXXXXX"  \# 自分のAPIキーに置き換えopenai.api\_key \= os.getenv("OPENAI\_API\_KEY") |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**Googleカスタム検索API**

| \# 検索APIの設定（自分のAPIキーと検索エンジンIDを入力）API\_KEY \= "YOUR\_GOOGLE\_API\_KEY"            \# 自分のAPIキーに置き換えSEARCH\_ENGINE\_ID \= "YOUR\_SEARCH\_ENGINE\_ID" \# 自分の検索エンジンIDに置き換えdef web\_search(query, num\_results=5):    \# Google Custom Search JSON APIのエンドポイントを構築    url \= (f"https://www.googleapis.com/customsearch/v1?key={API\_KEY}"           f"\&cx={SEARCH\_ENGINE\_ID}\&q={query}\&num={num\_results}")    resp \= requests.get(url)    data \= resp.json()    \# 結果からタイトルとURLを抽出    results \= \[\]    for item in data.get("items", \[\]):        title \= item.get("title")        link \= item.get("link")        snippet \= item.get("snippet")        results.append({"title": title, "link": link, "snippet": snippet})    return results\# 検索実行例query \= "最新のAI技術 動向"results \= web\_search(query, num\_results=3)for i, res in enumerate(results, start=1):    print(f"\[{i}\] {res\['title'\]} \- {res\['link'\]}") |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**Webページの取得と記事本文の取得**

| \# 検索結果の一番目のページを取得if not results:    raise Exception("検索結果が見つかりません。")target\_url \= results\[0\]\['link'\]page\_resp \= requests.get(target\_url)page\_resp.raise\_for\_status()  \# ページ取得が成功したか確認\# BeautifulSoupでHTML解析soup \= BeautifulSoup(page\_resp.text, 'html.parser')\# 方法1: \<p\>タグのテキストを全て結合して取得paragraphs \= \[p.get\_text() for p in soup.find\_all('p')\]article\_text \= '\\n'.join(paragraphs)\# 確認のため抽出したテキストの最初の一部を表示print(article\_text\[:500\]) |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**全テキストから不要タグを除外**

| \# 上級: 全テキストから不要タグを除外する例text\_elements \= soup.find\_all(text=True)blacklist \= \['\[document\]', 'noscript', 'header', 'html', 'meta',              'head', 'input', 'script', 'style'\]  \# 除外するタグfiltered\_text \= \[\]for element in text\_elements:    if element.parent.name not in blacklist:        filtered\_text.append(element.strip())clean\_text \= ' '.join(filtered\_text) |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**GPT-4による要約生成**

| \# GPT-4を使って記事内容を要約system\_prompt \= "あなたは有能なリサーチアシスタントです。与えられた記事の内容をわかりやすく要約してください。"user\_prompt \= f"記事の内容:\\n{article\_text}\\n\\n上記の内容を日本語でわかりやすく要約してください。"response \= openai.ChatCompletion.create(    model="gpt-4",    messages=\[        {"role": "system", "content": system\_prompt},        {"role": "user", "content": user\_prompt}    \],    temperature=0.7,  \# 必要に応じて応答の多様性を調整    max\_tokens=1000   \# 要約に割り当てるトークン数の上限)summary\_text \= response\['choices'\]\[0\]\['message'\]\['content'\]print("=== 要約結果 \===")print(summary\_text) |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**レポートの章構成と内容生成**

| \# 既に得られた要約結果を使用して、各セクションの内容をGPT-4に作成させるoverview\_prompt \= f"上記の要約を基に、記事全体の概要を2〜3文で書いてください。"findings\_prompt \= f"上記の要約を基に、その記事から得られる主な発見や重要ポイントを3つ箇条書きで挙げてください。"details\_prompt \= f"上記の要約を基に、記事の詳細な内容説明を段落で書いてください。"overview\_resp \= openai.ChatCompletion.create(    model="gpt-4",    messages=\[{"role": "user", "content": overview\_prompt}\],    max\_tokens=300)overview\_text \= overview\_resp\['choices'\]\[0\]\['message'\]\['content'\]findings\_resp \= openai.ChatCompletion.create(    model="gpt-4",    messages=\[{"role": "user", "content": findings\_prompt}\],    max\_tokens=300)findings\_text \= findings\_resp\['choices'\]\[0\]\['message'\]\['content'\]details\_resp \= openai.ChatCompletion.create(    model="gpt-4",    messages=\[{"role": "user", "content": details\_prompt}\],    max\_tokens=1000)details\_text \= details\_resp\['choices'\]\[0\]\['message'\]\['content'\]print("概要:\\n", overview\_text)print("主な発見:\\n", findings\_text)print("詳細情報:\\n", details\_text\[:200\], "...") |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**参考文献リストの作成**

| \# 参考文献リスト作成（タイトルとURLのペア）references \= \[\]page\_title \= soup.title.string if soup.title else "Webページ"references.append({"title": page\_title, "url": target\_url})\# 他に参考にしたURLがあれば追加可能\#print(references) |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |

**HTMLレポートの組み立てとPDF変換**

| \# 箇条書き（主な発見）を\<li\>タグに整形findings\_lines \= \[line.strip() for line in findings\_text.splitlines() if line.strip()\]findings\_html \= "\<ul\>\\n" \+ "\\n".join(f"  \<li\>{line.lstrip('-・')}\</li\>" for line in findings\_lines) \+ "\\n\</ul\>"\# 参考文献を\<li\>タグに整形（番号付きリスト\<ol\>を使用）references\_html \= "\<ol\>\\n"for ref in references:    title \= ref\['title'\]    url \= ref\['url'\]    references\_html \+= f"  \<li\>\<a href='{url}'\>{title}\</a\>\</li\>\\n"references\_html \+= "\</ol\>"\# HTML全文の組み立てhtml\_content \= f"""\<html\>\<head\>\<meta charset="UTF-8"\>\<style\>  body {{ font-family: "IPAexGothic", sans-serif; }}  h1, h2 {{ color: \#2e6c80; }}\</style\>\</head\>\<body\>\<h1\>リサーチレポート\</h1\>\<h2\>概要\</h2\>\<p\>{overview\_text}\</p\>\<h2\>主な発見\</h2\>{findings\_html}\<h2\>詳細情報\</h2\>\<p\>{details\_text}\</p\>\<h2\>参考文献\</h2\>{references\_html}\</body\>\</html\>"""\# HTML構造を確認（必要ならprint）\#print(html\_content) |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| \# HTMLコンテンツをPDFファイルに変換output\_pdf \= "research\_report.pdf"pdfkit.from\_string(html\_content, output\_pdf)\# PDFが正常に生成されたか確認import osprint(output\_pdf, "生成済み:", os.path.exists(output\_pdf)) |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

**レポートの出力**

| from google.colab import files\# PDFをローカルにダウンロードfiles.download(output\_pdf)  \# （オプション）PDFをGoogle Driveにコピーして保存from google.colab import drivedrive.mount('/content/drive')\!cp {output\_pdf} /content/drive/MyDrive/ |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

### **3.3 OpenAi Agent SDKを用いた場合**

| \# 1\. 必要ライブラリのインストールとwkhtmltopdfのセットアップ\!pip install openai-agents requests beautifulsoup4 pdfkit\!apt-get update \-y\!apt-get install \-y wkhtmltopdf |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| \# 2\. インポートとAPIキーの設定import osimport requestsimport pdfkitfrom bs4 import BeautifulSoup\# 最新のAgents SDKモジュールをインポートimport openaifrom openai.agents import Agent, Runner, function\_tool, set\_default\_openai\_key\# 最新のSDKでは、set\_default\_openai\_key()を使ってAPIキーを設定します（コマンドラインからexportする必要はありません）set\_default\_openai\_key("sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")  \# ←ご自身のAPIキーに置き換えてください |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |

| \# 3\. ツールの定義\# ① Web検索ツール：Google Custom Search API を利用@function\_tooldef web\_search(query: str) \-\> str:    """    Google Custom Search APIでクエリを実行し、上位5件の検索結果（タイトル、URL、スニペット）を返します。    """    GOOGLE\_CSE\_API\_KEY \= "YOUR\_GOOGLE\_API\_KEY"   \# ご自身のGoogle APIキーに置き換え    GOOGLE\_CSE\_ID \= "YOUR\_SEARCH\_ENGINE\_ID"       \# ご自身の検索エンジンIDに置き換え    url \= (f"https://www.googleapis.com/customsearch/v1?"           f"key={GOOGLE\_CSE\_API\_KEY}\&cx={GOOGLE\_CSE\_ID}\&q={query}\&num=5")    resp \= requests.get(url)    data \= resp.json()    items \= data.get("items", \[\])    if not items:        return "検索結果がありませんでした。"    results\_text \= ""    for i, item in enumerate(items, start=1):        title \= item.get("title", "")        link \= item.get("link", "")        snippet \= item.get("snippet", "")        results\_text \+= f"\[{i}\] {title}\\nURL: {link}\\nSnippet: {snippet}\\n\\n"    return results\_text\# ② 記事取得ツール：指定URLからHTMLを取得し、本文（\<p\>タグ）を抽出@function\_tooldef fetch\_article(url: str) \-\> str:    """    URLのページを取得し、HTML内の\<p\>タグから本文テキストを抽出して返します。    """    try:        resp \= requests.get(url, timeout=10)        resp.raise\_for\_status()    except Exception as e:        return f"ページ取得に失敗しました: {str(e)}"    soup \= BeautifulSoup(resp.text, 'html.parser')    paragraphs \= soup.find\_all("p")    text \= "\\n".join(\[p.get\_text().strip() for p in paragraphs if p.get\_text().strip()\])    if not text:        return "本文を抽出できませんでした。"    return text\# ③ PDF生成ツール：HTML文字列からPDFを生成@function\_tooldef make\_pdf(html\_content: str) \-\> str:    """    HTMLコンテンツをPDFに変換し、ファイル名を返します。    """    output\_pdf \= "report.pdf"    try:        pdfkit.from\_string(html\_content, output\_pdf)    except Exception as e:        return f"PDF生成に失敗しました: {str(e)}"    return f"PDFを生成しました: {output\_pdf}" |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |

| \# 4\. エージェントの定義\# 今回は、リサーチエージェントとして、以下の手順を実行するように指示します：\# 1\. ユーザーのトピックに基づいてweb\_searchツールで最新情報を検索\# 2\. 必要に応じて、fetch\_articleツールで詳細情報を取得\# 3\. 取得した情報を元に、章構成（概要・主な発見・詳細情報・参考文献）のレポートを作成\# 4\. 最終的に、レポートのHTMLをmake\_pdfツールでPDFに変換し、その結果を返すagent\_instructions \= """あなたはリサーチエージェントです。以下の手順で動作してください。1\. ユーザーから与えられたトピックに対して、まずweb\_searchツールを使い、最新情報の検索結果を取得してください。2\. 検索結果から関連性の高い記事のURLを見つけ、fetch\_articleツールで記事本文を取得してください。3\. 取得した情報をもとに、概要、主な発見、詳細情報、参考文献の4つの章に分けたレポートを作成してください。4\. 作成したレポートをHTML形式に整形し、make\_pdfツールを使ってPDFを生成してください。最終的に、PDF生成の結果（ファイル名など）を出力してください。"""agent \= Agent(    name="ResearchAgent",    instructions=agent\_instructions,    model="gpt-4o",    tools=\[web\_search, fetch\_article, make\_pdf\]) |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| \# 5\. エージェントの実行\# Runner.run\_sync() を使って、エージェントにタスクを指示します。result \= Runner.run\_sync(agent, "最新のAI技術の進展について調査し、詳細なレポートをPDFにまとめてください。")print("最終出力:")print(result.final\_output) |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |

| \# 6\. 生成されたPDFのダウンロード（Google Colabの場合）from google.colab import filesfiles.download("report.pdf") |
| :------------------------------------------------------------------------------------------------------------------- |

## **4\. まとめと今後の展望**

### **4.1 Agent SDK活用のメリット**

従来の方法とAgent SDKを使用した方法を比較すると、以下のメリットが明らかになる：

* **コード量の大幅削減**
  * Agent SDKではエージェントのハイレベルな指示と必要なツールを定義するだけで済む
  * 各種ロジックの実装が大幅に簡略化される
* **開発効率の向上**
  * 段階的なタスク処理の流れをエージェントに委任できる
  * エージェントが自律的に判断して適切なツールを呼び出す
* **機能拡張の容易さ**
  * 新しいツールの追加が容易（関数を定義してデコレータを付与するだけである）
  * エージェントの指示を修正するだけで新しい動作パターンに対応可能である

### **4.2 今後の展望**

* **エンタープライズ領域での普及**:
  * 社内システムとの連携によるカスタムエージェントの増加
  * 複雑なワークフローの自動化へのAgent SDK活用
* **マルチモーダル対応**:
  * 画像・音声・動画などのマルチモーダルデータを処理するエージェントの開発
  * 視覚的・聴覚的情報を含む複雑なタスクの自動化
* **エージェント間連携の発展**:
  * 専門化されたエージェント間の協調による複雑タスクの解決
  * チーム型エージェントシステムの構築フレームワークの整備
* **学習・適応型エージェントの登場**:
  * フィードバックから学習するエージェントの開発
  * ユーザーの好みや行動パターンに適応するパーソナライズドエージェント

Agent SDKは、AIエージェント開発の民主化と標準化を促進し、開発者がより少ない労力で高度なAI機能を実装できる環境を提供している。今後のAI開発において中心的な役割を果たすことが期待される。
